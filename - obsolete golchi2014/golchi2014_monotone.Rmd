---
title: "golchi2014_monotone"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Example 1

```{r setup}

# monotone increasing function
ytrue <- function(x) {log(20*x + 1)}  # x > -1/20

x <- c(0, 0.1, 0.2, 0.3, 0.4, 0.9, 1)   # inputs where fcn evaluated
y <- ytrue(x)

xprime <- c(0.42, 0.47, 0.52, 0.57, 0.62, 0.67, 0.72, 0.77, 0.82, 0.87)  # derivative set
xstar <- seq(0, 1, length.out = 50)  # prediction set
 
plot(x, ytrue(x), pch = 16, cex = 2)
points(xstar, ytrue(xstar), pch = 16, col = "green")
curve(ytrue, add = TRUE)

tauT <- 10e-6

```

```{r}
# covariance functions
## dim(x) = 1

r <- function(xi, xj, sig2 = 1, ltrans = 1) {
  
  # g(.): from Matern classs of covariance functions
  ## only need g = g_k because dim(x) = 1
  ## modified bessel function of the second kind
  ## https://stat.ethz.ch/pipermail/r-help/2002-December/027850.html
  g <- function(xi, xj, ltrans) {
    
    # 2^(1 - lambda) / gamma(lambda) * 
    #   ( sqrt(2 * lambda) * abs(xi - xj) / l ) ^ lambda *
    #   besselK(sqrt(2 * lambda) * abs(xi - xj) / l, nu = lambda)
    
    ## version where ltrans = sqrt(2 * lambda) / l
    lambda <- 5/2
    2^(1 - lambda) / gamma(lambda) * 
      ( ltrans * abs(xi - xj) ) ^ lambda *
      besselK( ltrans * abs(xi - xj), nu = lambda)
    
  }
  
  if (xi == xj) {
    return(sig2)
  } else {
    return(sig2 * g(xi, xj, ltrans))
  }
  
}

# FROM NOW ON l = ltrans #############################################################

# R matrix

Rmat <- function(x, s2 = 1, l = 1) {
  
  R <- matrix(NA, nrow = length(x), ncol = length(x))
  
  for (i in 1:length(x)) { # I know this is inefficient
    for (j in 1:length(x)) {
      
        R[i, j] <- r(xi = x[i], xj = x[j], sig2 = s2, l = l)
      
    }
  }
  
  return(R)
}

Rmat(x)
## should the diags be NaN ??? qqq I think they should be the sig2???

solve(Rmat(x))  # R inverse




# predicting for 1 new input xstar

# helpers
## qqq pass in sig2, l???
r.xstarX <- function(xstar, x) {
  
  mu <- rep(NA, length(x))  # mean vector for lik1
  for (i in 1:length(x)) {
    mu[i] <- r(xi = xstar, xj = x[i])
  }
  
  return(mu)
  
}

# r.xstar(xstar = xstar[3], x)

```

```{r naive joint posterior}
# setup posterior kernel
library(mvtnorm)

post.kernel <- function(xstar, ystar, l, sig2, y, x) {
  
  # likelihood1
  R <- Rmat(x = x, s2 = sig2, l = l)
  Rinv <- solve(R)
  r.xstar <- r.xstarX(xstar = xstar, x = x)
  
  mu.xstar <- r.xstar %*% Rinv %*% y
  tau2.xstar <- r.xstar %*% Rinv %*% r.xstar
  
  likelihood1 <- dmvnorm(ystar, mean = mu.xstar, sigma = tau2.xstar)
  
  # likelihood2
  likelihood2 <- dmvnorm(y, mean = rep(0, length(x)), sigma = R)  # qqq why is this zero...
  
  # priors
  ## recall that l <- ltrans = sqrt(2 * lambda) / l ~ chi(1)
  lambda <- 5/2
  priors <- dchisq(l, df = 1) *
    dchisq(sig2, df = 5)
  
  return( likelihood1 * likelihood2 * priors )
}

# MH

MH.step <- function(Zold, newx, y, x) {
  
  Znew <- c(abs(rnorm(1, mean = Zold[1], sd = 0.01)),
            abs(rnorm(1, mean = Zold[2], sd = 0.01)),
            rnorm(1, mean = Zold[3], sd = 0.1))
  HR <- post.kernel(xstar = newx, ystar = Znew[3], l = Znew[1], sig2 = Znew[2], y = y, x = x) / 
    post.kernel(xstar = newx, ystar = Zold[3], l = Zold[1], sig2 = Zold[2], y = y, x = x)
  
  if (HR > runif(1)) {
    Zi <- Znew
  } else {
    Zi <- Zold
  }
  
  return(Zi)
}

MH.naive <- function(N = 100, newx = xstar[1], y, x) {
  # N: chain length
  Z <- matrix(NA, nrow = N, ncol = 3)
  
  # 1. initialise
  Z0 <- c(0.5, 2, ytrue(newx))
  
  # 2. HR
  ## first step
  Z[1,] <- MH.step(Zold = Z0, newx = newx, y = y, x = x)
  for (i in 2:nrow(Z)) {
    Z[i,] <- MH.step(Zold = Z[i-1,], newx = newx, y = y, x = x)
  }

  return(Z)
}

# debugging
ystar = Znew[3]; l = Znew[1]; sig2 = Znew[2]
test <- MH.naive(N = 100, newx = xstar[1], y = y, x = x)
```

