meetings:
	170508:
		- lock for code for golchi 2014 monotone emulation  @done
			none from google; something on her personal site but not exactly
		- priors can determine whether the answers you get out are sensible (not really bayesian but whatever)
		- can use sample unscaled variance of the observations to decide how to distribute variance across different parts of the model. i.e. say certain amount on physics model, some on discrepancy, some on random error
		- scaled random walk just needs to be a symmetric distribution
	170512:
		- GP = regression with correlated errors
		- just accept that the derivative of a GP is a GP
		- can choose GP correlation function such that a GP can approximate non-differentiable stuff
			- still need knowledge of what the function should look like or lots of samples though
		- standardise data... might be close enough GP
	170519:
		- matern because of numerical stability (also reviewer was an ass) compared to squared exponential
			- squared exponential can lead to to cov matrices with columns that are too similar => computer issues (rounding error), i.e. it things the matrix is not positive semidefinite
			- read the welch paper black box blah blah
				- something about a nugget so that the numerical instability can be fixed
		- yes if hastings ratio numerator/denom too close to zero, can log both sides
	170601:
		# weird length-scale parameter from mcmc
			recall length-scale l shows up as sqrt(5) / l
			but then it's really hard to have a justifiable prior => transform it so that instead of getting exp(-sqrt(5) / l * distance), we have P^(distance) 
				i.e. variable transformation so instead of support(l) > 0, we have support(P) \in (0, 1)
				easier to put a prior on P than l
		# "parameter" choices
			you include (y*, y') as parameters in the posterior only when you know which new inputs where you want prediction
			typically you don't know this so the posterior is just a function of the GP parameters 
			for Golchi 2014 EX1. you do know y*, y' etc. so it's fine to include it in the posterior
		# mcmc output
			so you've done MCMC and have chains for the GP parameters
				look at the histograms to see the distributions!!
			the correct way to get sample paths is to 
				1. sample from posterior dis. for each parameter
					it might be ok to use a point estimate based on your MCMC chain IF the distribution of the parameter is unimodal with small variance
						DB prefers posterior median instead of the posterior mean because it's unlikely that the distribution should be symmmetric
					sometimes if it's hard to get a distribution of the GP parameter i.e. you didn't explore the space a lot etc. maybe use the posterior mode
					i.e. if you use a point estimate to get your GP sample paths this way, then you're not accounting for any of the variability on your GP parameters
						consider a CI vs. a prediction interval
				2. use the sampled parameters to sample from the MVN appropriately 
						(apparently the mean and covariance formulas [y* | y, GP parameters] are the kriging equations)
		# numerical instability in the matern covariance
			recall: can fix via the nugget, or the (A + t(A) ) / 2 trick; however...
			## don't use the solve() or inv()
			- look up the cholesky decomposition (i.e. gives S^-0.5)
				or QR decomposition?
				recall SVD can give eigenvalues for non-square matrices
			## build your on sampler from the MVN distribution
			i.e. the package one probably sucks
			- build your on sampler from the MVN distribution
				1. sample y where yi ~ N(0, 1) independently
				2. S^-0.5 y = y', where y' ~ N(0, S)
			## squared exponential vs. matern
				consider that squared exp is basically a more strict constraint on the function space
					because squared exp is infinitely differentiable
				matern only has a few derivatives that exist so it is a more relaxed constraint
					=> using the matern leads to fewer numerical instability issues
		# range of problems
			recall in Golchi 2014 EX1, the x ranges from (0, 1)
			apparently this is quite standard (i.e. even for vector x, all x_i range from 0, 1) 
				remember i can be thought of as an index for some covariate value, hence some of the GP parameter are like a regression coefficient
				https://stats.stackexchange.com/questions/277176/gaussian-process-and-correlation
			since the standardisation just ends up in the GP parameters for example it gets absorbed by the length-scale parameter 
		# acceptance rate
			- email DB about JASA paper (Todd Graves, JASA tuning MCMC) @done
			- check EX1. and make those plots for some initial values;  @done
				given initial value, x-axis = step-width for proposal, y-axis = acceptance rate i.e. inverse regression to choose step size based on acceptance rate
				- report to DB aiming for 30-40%
		- do the SMC finally
	170620:
		# acceptance rate for eta0
			apparently aiming for 20-50% (combination 8 looks good enough)
		# inlcuding y* and y ' as unknown parameters
			think of all Bayesian analysis as problems with missing/unknown values
				the jones, schonlau paper alternate proof for the kriging equations derives them by treating y* as a missing value
			when the problem is high-dimensional
				=> better to include (y*, y') as unknowns because to have a "general grid" of partial derivative values over the whole space of y values would be computationally horrifying
				i.e. the covariance matrix would be huge and thus horrible to invert
				in contrast, knowing where we want to make predictions, we can just specify the derivative information around those points, thus reducing the overall number of unknowns hence smaller covariance matrix
			when the problem is low-dimensional
				=> it's fine to just have y' as unknowns because y* | y, y' etc. computation is not bad
			Alg 1 from golchi is the algorithm for high-dimensional problems
		particle filtering is a kind of SMC (though it seems like internet says that particle filter == SMC)
			bootstrap filtering is basically what Alg 1 uses
		# diagnosing MCMC
			divide chain into chunks ("bits"), for each chunk sort, plot qq plot (?)
		# general advice
			- go to the machine learning things
			- improve github CV via app or something
		# TODO
			- SCMC omg
				- use golchi's structure for reference
			- figure out whether I want CRA or los alamos
			- read more on adaptive MCMC/SMC
				- what is the acceptance rate exactly (particle survival?) in the context of SMC?
				maude/dave's hack: for first third of iterations in chain, check that acceptance rate is within desired range, if not multiply step size by target rate/current rate
			- add case study to CV @done
			- email prof welch (bcc jabed?) @done
			- make mvn functions coded part of the package actually (export etc.) @done
	170707:
		- check my covariance functions with shirin's
			interp. first derivative as distance goes to zero, derivative value and non derivative value uncorrelated
			first deriv covariance does not have to be positive definite because think k(xstar, x) is not necessarily
				and diagonals are no longer variances
			but second deriv should be because the diagonals are actually variance of the derivative 
		- check the numerical slope with GP slope
			no need to have the awkward gap in sampled points... just take 10 equally spaced points between 0 to 1
		- what happens when I also set the means corresponding to derivatives to 0?
		nugget size of 10e-5 is fine (so my 10e-6 is really fine)
	170711:
		gaussian process stuff
			think about the universe i.e. we really only have a single realisation (sample size 1) which means we have no measure of noise
			also any discrepancy = delta term
			if we have physical measurements, that's where discrepancy and noise come in 
			consider that it's not possible to actually get the true GP parameters because the true GP parameters are for the "population" but since we only have a single set of numbers = a single sample/realisation of the GP, the GP parameters we estimate are really just specific for that single sample
		yes the derivative GP covariance matrix is symmetric (says michael and derek); i.e. eqn (3.8) is symmetric BUT our covMatrix function is really just for calculating the lower triangle!
			shirin's matcorder is missing a negative sign
			the appendix is wrong for the second derivative of the matern (no divided by 2, no sign term)
		- fix matern2 i.e. what are you even calculating?? @done
		- change all calls to covMatrix() @due @done
		- finish fixing ystar yprime update
		- sampling l
				delta <- rnorm(1, 0, ql)
				newl <- l[i] + delta
				if (newl > 0) 
				BUT 
				ratio <- (log(pnorm(l[i], 0, 0.01)) + lpnum) - (log(pnorm(newl, 0, 0.01)) + lpden)
			- why is the pnorm sd 0.01 and not ql?
		- sampling ystar yprime
				g0 = sum(log(pnorm(new[(n+1):len]/nu)))
				g1 = sum(log(pnorm(z[i,][(n + 1):len] / nu)))
				ratio = lmvnlik(c(new),mean,covmat) + g0 - lmvnlik(z[i,], mean, covmat) - g1
	170718:
		so it turns out to make the variability go down within 10 time steps,
			always resample according to the weights no matter what
			don't multiply new weights by the old ones
			never actually propose anything new
			
questions:
	- why only allow l > 0.05 proposals?
		probably for numerical stability reasons; also length scale close to zero is stupid
	- why ginv() in MCMC_unconstrained?
	- what's going on with the covariances when generating initial values for MCMC_unconstrained I think
			Rdq <- 0.001 * Rd  # why the 0.001???
			cc <- t(as.matrix(covmat[1:n, (n + 1):(n + m)]))
			RR <- covmat[1:n, 1:n]
			mean <- t(C) %*% Rinv %*% y_obs
			mean1 <- mean[(n + 1):(n + m)]  # derivative values I think
			mean1[mean1 < 0] <- 0  # ???
	- nuseq definition? (defined as reciprocal of tau sequence)
		- why power of 5?
		- why use the reciprocal?
	- why are the weights logged? i.e. sum log(cdf() + cdf); end up taking exponent anyway @done
		proably due to multiplying small numbers => underflow errors
		yes that is the case: http://www.nowozin.net/sebastian/blog/effective-sample-size-in-importance-sampling.html
	- why isn't reweighting following the if statement? i.e. why is resampling always done? 
	- step size for sampling l is sd(l) ?? why??
	- what does acceptance rate mean for S(C)MC?
		- how did you assess this?
	- when set M = 3 for 5 particles, at t = 3, all the particle weights drop to zero
	- yprime issues
		- the estimates suck
		- the signs are wrong
	- ystar issues
		- why are the estimates too good
		it's because the length scale is huge... consider that it's over 5, but then x only goes from 0 to 1... 
	- nugget size (shirin uses 1e-5)
		- particle initialisation: 10e-8 not enough; 10e-7 (5/15 warnings)
		- main SCMC: 10-8 some warnings? 10e-7
		- is it better to choose a larger nugget to kill off all warning messages or are a few ok?
	- speed
		- expectations?